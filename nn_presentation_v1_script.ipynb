{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darstellung unserer Daten\n",
    "\n",
    "Wie bereits erwähnt, kann jedes Trainingsbeispiel durch einen Vektor mit 784 Elementen dargestellt werden, die jedem der 784 Pixel des Bildes entsprechen.\n",
    "\n",
    "Diese Vektoren können in einer Matrix gestapelt werden, um vektorisierte Berechnungen durchzuführen. Das heißt, anstatt eine for-Schleife zu verwenden, um alle Trainingsbeispiele durchzugehen, können wir den Fehler aus allen Beispielen auf einmal mit Matrixoperationen berechnen.\n",
    "\n",
    "In den meisten Kontexten, auch beim maschinellen Lernen, werden diese Vektoren als Zeilen der Matrix gestapelt, so dass die Matrix die Dimensionen $m: \\text{rows} \\times n : \\text{columns}$, wobei $m$ die Anzahl der Trainingsbeispiele und $n$ die Anzahl der Merkmale ist, in diesem Fall 784. Um unsere Berechnungen zu vereinfachen, transponieren wir diese Matrix und geben ihr stattdessen die Dimensionen $n \\times m$, wobei jede Spalte einem Trainingsbeispiel und jede Zeile einem Trainingsmerkmal entspricht.\n",
    "\n",
    "![nn2](https://www.samsonzhang.com/blog/2020-11/nn2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesen Darstellungen im Hinterkopf können wir nun die Gleichungen für die Vorwärtspropagation schreiben.\n",
    "\n",
    "Zunächst berechnen wir die nicht aktivierten Werte der Knoten in der ersten verborgenen Schicht, indem wir $W^{[1]}$ und $b^{[1]}$ auf unsere Eingabeschicht anwenden. Wir nennen die Ausgabe dieser Operation $Z^{[1]}$:\n",
    "\n",
    "$Z^{[1]} = W^{[1]}X+b^{[1]}$\n",
    "\n",
    "Erinnern Sie sich, dass $X$ die Dimensionen $784\\times m$ hat, und $W^{[1]} : 10\\times784$. $W^{[1]}X$ ist das Punktprodukt zwischen den beiden, was eine neue Matrix der Dimensionen $10\\times m$ ergibt. Das mag auf den ersten Blick etwas seltsam erscheinen, aber man muss sich das so vorstellen: Jede Spalte dieser Matrix entspricht den nicht aktivierten Werten für die Knoten in der ersten verborgenen Schicht, wenn sie für ein Trainingsbeispiel ausgeführt wird, so dass die gesamte Matrix die Ausführung des ersten Schritts der Vorwärtspropagation für alle Trainingsbeispiele gleichzeitig darstellt. Dies ist wesentlich effizienter als eine for-Schleife und wurde bereits als \"vektorisierte Implementierung\" bezeichnet.\n",
    "\n",
    "Unser Bias-Term $b^{[1]}$ hat die Dimensionen $10\\times1$, aber wir wollen, dass dieselbe Spalte von Biases auf alle $m$ Spalten von Trainingsbeispielen angewendet wird, also wird $b^{[1]}$ bei der Berechnung von $Z^{[1]}$ effektiv in eine Matrix der Dimensionen $10\\times m$ übertragen, die den Dimensionen von $W^{[1]}X$ entspricht.\n",
    "\n",
    "Wir müssen jedoch noch eine weitere Berechnung durchführen, bevor wir zur nächsten Schicht übergehen, und zwar die Anwendung einer nichtlinearen Aktivierung auf $Z^{[1]}$. Was bedeutet das, und warum müssen wir das tun?\n",
    "\n",
    "Stellen Sie sich vor, dass wir jetzt nichts mit $Z^{[1]}$ anstellen, sondern es mit $W^{[2]}$ multiplizieren und $b^{[2]}$ hinzufügen, um den Wert für die nächste Schicht zu erhalten. $Z^{[1]}$ ist eine Linearkombination der Eingangsmerkmale, und die zweite Schicht wäre eine Linearkombination von $Z^{[1]}$, also immer noch eine Linearkombination der Eingangsmerkmale. Das bedeutet, dass unsere versteckte Schicht im Wesentlichen nutzlos ist und wir lediglich ein lineares Regressionsmodell erstellen.\n",
    "\n",
    "Um diese Reduktion zu verhindern und die Komplexität unserer Schichten zu erhöhen, lassen wir $Z^{[1]}$ durch eine nichtlineare Aktivierungsfunktion laufen, bevor wir sie an die nächste Schicht weitergeben. In diesem Fall verwenden wir eine Funktion namens \"rectified linear unit\" (ReLU):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these representations in mind, we can now write the equations for forward propagation.\n",
    "\n",
    "First, we'll compute the unactivated values of the nodes in the first hidden layer by applying $W^{[1]}$ and $b^{[1]}$ to our input layer. We'll call the output of this operation $Z^{[1]}$:\n",
    "\n",
    "$Z^{[1]} = W^{[1]}X+b^{[1]}$\n",
    "\n",
    "Remember that $X$ has dimensions $784\\times m$, and $W^{[1]} : 10\\times784$. $W^{[1]}X$ is the dot product between the two, yielding a new matrix of dimensions $10\\times m$. This may seem a little strange at first, but think of it this way: each column of this matrix corresponds to the unactivated values for the nodes in the first hidden layer when carried out for one training example, so the entire matrix represents carrying out the first step of forward propagation for all training examples at the same time. This is much more efficient than a for loop, and is what was referred to earlier as a \"vectorized implementation.\"\n",
    "\n",
    "Our bias term $b^{[1]}$has dimensions $10\\times1$, but we want the same column of biases to be applied to all $m$ columns of training examples, so $b^{[1]}$is effectively broadcast into a matrix of dimensions $10\\times m$ when calculating $Z^{[1]}$, matching the dimensions of $W^{[1]}X$.\n",
    "\n",
    "We need to do one more calculation before moving on to the next layer, though, and that's applying a non-linear activation to $Z^{[1]}$. What does this mean, and why do we have to do it?\n",
    "\n",
    "Imagine that we didn't do anything to $Z^{[1]}$ now, and multiplied it by $W^{[2]}$ and added $b^{[2]}$ to get the value for the next layer. $Z^{[1]}$ is a linear combination of the input features, and the second layer would be a linear combination of $Z^{[1]}$, making it still a linear combination of the input features. That means that our hidden layer is essentially useless, and we're just building a linear regression model.\n",
    "\n",
    "To prevent this reduction and actually add complexity with our layers, we'll run $Z^{[1]}$ through a non-linear activation function before passing it off to the next layer. In this case, we'll be using a function called a rectified linear unit, or ReLU:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darstellung von Gewichten und Biases\n",
    "\n",
    "Schauen wir uns nun unser neuronales Netz an. Zwischen jeweils zwei Schichten gibt es eine Reihe von Verbindungen zwischen jedem Knoten in der vorherigen Schicht und jedem Knoten in der folgenden Schicht. Das heißt, es gibt eine Gewichtung $w_{i,j}$ für jeden $i$ in der Anzahl der Knoten in der vorherigen Schicht und jeden $j$ in der Anzahl der Knoten in der folgenden Schicht.\n",
    "\n",
    "Es ist also naheliegend, unsere Gewichte als eine Matrix der Dimensionen $n^{[l]}\\times n^{[l-1]}$ darzustellen, wobei $n^{[l-1]}$ die Anzahl der Knoten in der vorherigen Schicht und $n^{[l]}$ die Anzahl der Knoten in der folgenden Schicht ist. Nennen wir diese Matrix $W^{[l]}$, die der Schicht $l$ unseres Netzes entspricht. $W^{[1]}$ ist zum Beispiel eine $10\\times784$-Matrix, die uns von den 784 Knoten der Eingabeschicht zu den 10 Knoten der ersten verborgenen Schicht führt. $W^{[2]}$ wird die Dimensionen $10\\times10$ haben.\n",
    "\n",
    "Biases sind einfach konstante Terme, die zu jedem Knoten der folgenden Schicht hinzugefügt werden, so dass wir sie als Matrix mit den Dimensionen $n^{[l]} \\times 1$ darstellen können. Nennen wir diese Matrizen $b^{[l]}$, so dass $b^{[1]}$ und $b^{[2]}$ beide die Dimensionen $10\\times1$ haben."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "\n",
    "Now, we'll go the opposite way and calculate how to nudge our parameters to carry out gradient descent.\n",
    "\n",
    "Mathematically, what we're actually computing is the derivative of the loss function with respect to each weight and bias parameter. For a softmax classifier, we'll use a cross-entropy loss function:\n",
    "\n",
    "$J(\\hat{y}, y) = -\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "Here, $\\hat{y}$ is our prediction vector. It might look like this:\n",
    "\n",
    "$\\begin{bmatrix} 0.01 \\ 0.02 \\ 0.05 \\ 0.02 \\ 0.80 \\ 0.01 \\ 0.01 \\ 0.00 \\ 0.01 \\ 0.07 \\ \\end{bmatrix}$\n",
    "\n",
    "$y$ is the one-hot encoding of the correct label for the training example. If the label for a training example is 4, for example, the one-hot encoding of $y$ would look like this:\n",
    "\n",
    "$\\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ \\end{bmatrix}$\n",
    "\n",
    "Notice that in our sum $\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$, $y_i = 0$ for all $i$ except the correct label. The loss for a given example, then, is just the log of the probability given for the correct prediction. In our example above, $J(\\hat{y}, y) = -\\log(y_4) = -\\log(0.80) \\approx 0.097$. Notice that, the closer the prediction probability is to 1, the closer the loss is to 0. As the probability approaches 0, the loss approaches $+\\infty$. By minimizing the cost function, we improve the accuracy of our model. We do so by substracting the derivative of the loss function with respect to each parameter from that parameter over many rounds of graident descent:\n",
    "\n",
    "$W^{[1]} := W^{[1]} - \\alpha \\frac{\\delta J}{\\delta W^{[1]}} \\ b^{[1]} := b^{[1]} - \\alpha \\frac{\\delta J}{\\delta b^{[1]}} \\ W^{[2]} := W^{[2]} - \\alpha \\frac{\\delta J}{\\delta W^{[2]}} \\ b^{[2]} := b^{[2]} - \\alpha \\frac{\\delta J}{\\delta b^{[2]}} \\ $\n",
    "\n",
    "Our objective in backprop is to find $\\frac{\\delta J}{\\delta W^{[1]}},\\frac{\\delta J}{\\delta b^{[1]}},\\frac{\\delta J}{\\delta W^{[2]}},$ and $\\frac{\\delta J}{\\delta b^{[2]}}$. For concision, we'll write these values as $dW^{[1]}, db^{[1]}, dW^{[2]},$and $db^{[2]}$. We'll find these values by stepping backwards through our network, starting by calculating $\\frac{\\delta J}{\\delta A^{[2]}}$, or $dA^{[2]}$. Turns out that this derivative is simply:\n",
    "\n",
    "$dA^{[2]} = Y - A^{[2]}$\n",
    "\n",
    "If you know calculus, you can take the derivative of the loss function and confirm this for yourself. (Hint: $\\hat{y} = A^{[2]}$)\n",
    "\n",
    "From $dA^{[2]}$, we can calculate $dW^{[2]}$ and $db^{[1]}$:\n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T} \\ dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$\n",
    "\n",
    "Then, to calculate $dW^{[1]}$ and $db^{[1]}$, we'll first find $dZ^{[1]}$:\n",
    "\n",
    "$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (Z^{[1]})$\n",
    "\n",
    "I won't explain all the details of the math, but you can get some intuitive hints at what's going on here by just looking at the variables. We're applying $W^{[2]T}$ to $dZ^{[2]}$, akin to applying the weights between layers 1 and 2 in reverse. Then, we perform an element-wise multiplication with the derivative of the activation function, akin to \"undoing\" it to get the correct error values.\n",
    "\n",
    "Since our activation function is ReLU, our derivative is actually pretty simple. Let's revisit our graph:\n",
    "\n",
    "![nn3](https://www.samsonzhang.com/blog/2020-11/nn3.png)\n",
    "\n",
    "When the input value is greater than 0, the activation function is linear with a derivative of 1. When the input value is less than 0, the activation function is horizontal with a derivative of 0. Thus, $g^{[1]\\prime}(Z^{[1]})$ is just a matrix of 1s and 0s based on values of $Z^{[1]}$.\n",
    "\n",
    "From here, we do the same calculations as earlier to find $dW^{[1]}$ and $db^{[1]}$, using $X$ in place of $A^{[1]}$:\n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T \\ dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$\n",
    "\n",
    "Now we've found all the derivatives we need, and all that's left is to update our parameters:\n",
    "\n",
    "$W^{[2]} := W^{[2]} - \\alpha dW^{[2]} \\ b^{[2]} := b^{[2]} - \\alpha db^{[2]} \\ W^{[1]} := W^{[1]} - \\alpha dW^{[1]} \\ b^{[1]} := b^{[1]} - \\alpha db^{[1]}$\n",
    "\n",
    "Here, $\\alpha$is our learning rate, a \"hyperparameter\" that we set to whatever we want. $\\alpha$ is distinguished from other parameters because, just like the number of layers in the network or the number of units in each layer, it's a value that we choose for our model rather than one that gradient descent optimizes.\n",
    "\n",
    "With that, we've gone over all the math that we need to carry out gradient descent and train our neural network. To recap: first, we carry out forward propagation, getting a prediction from an input image:\n",
    "\n",
    "$Z^{[1]} = W^{[1]} X + b^{[1]} \\ A^{[1]} = g_{\\text{ReLU}}(Z^{[1]})) \\ Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\ A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$\n",
    "\n",
    "Then, we carry out backprop to compute loss function derivatives:\n",
    "\n",
    "$dZ^{[2]} = A^{[2]} - Y \\ dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T} \\ dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}} \\ dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]}) \\ dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T} \\ dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$\n",
    "\n",
    "Finally, we update our parameters accordingly:\n",
    "\n",
    "$W^{[2]} := W^{[2]} - \\alpha dW^{[2]} \\ b^{[2]} := b^{[2]} - \\alpha db^{[2]} \\ W^{[1]} := W^{[1]} - \\alpha dW^{[1]} \\ b^{[1]} := b^{[1]} - \\alpha db^{[1]}$\n",
    "\n",
    "We'll do this process over and over again — the exact number of times, an iteration count that we again set ourselves — until we are satisfied with the performance of our model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
